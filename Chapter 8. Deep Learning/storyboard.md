# Chapter 8. 딥러닝

이러서 8장 딥러닝에 관한 내용입니다.

## 8.1 더 깊게

지금까지 / Batch Normalization을 이용해 한 번에 소량의 데이터로 학습시키기부터 / 일반적인 학습 형태인 행렬곱 연산과 편향과의 덧셈 / 또는 이미지 및 다차원 데이터를 위한 Convolution 계층, / 다음 층을 위해 신호로 변환하는 Sigmoid, ReLU 등, / 나아가 CNN에서 데이터를 줄이는 데 이용되는 Pooling, / init_weight을 위한 std 이용법, / Sigmoid: Xavier 초깃값, ReLU: he 초깃값, SGD, Adam, AdaGrad 등등을 배웠습니다. / 
이것들을 적절히 이용해 / 기존에 최적이자 최선이라고 알려진 / MNIST 데이터의 신경망을 구현해보고자 합니다.

### 8.1.1 더 깊은 신경망으로

기본적으로 첫번쨰 층에는 Convolution, ReLU로 이루어지고, / 두번째 층에는 Convolution, ReLU, Pooling이 한 단위를 이룹니다. / 이 확장된 Convolutional 계층이 총 3번 반복되고 / 이후 Affine, ReLU 계층에 Dropout이 추가되고, / Dropout이 추가된 완전연결 계층을 거쳐 output의 label로 분류됩니다.

이 신경망은 특이하게도 / Convolution 계층에서 이용하는 filter의 크기가 3으로 모두 동일합니다. / 그리고 채널 수가 / 16, 16, 32, 32, 64, 64로 pooling 이후의 / Convolution 계층을 만나면서 / 채널 수가 2배로 증가하는 특징이 있습니다. / 또, 모든 Activation function은 ReLU이며, / 이를 더 효과적으로 이용하고자 / He 초깃값 이용합니다. / 또, 완전 연결 계층에서만 Dropout을 사용했으며 / 최적의 Loss를 찾는 방법으로 Adam을 이용합니다.

이 신경망은 / 놀랍게도 MNIST 데이터에 대해 99.38%의 정확도를 보입니다. / 
이때 잘못 예측한 것들을 보면

사람이 보기에도 / 필기 습관에 따라 다르게 인식할 만한 / 글자들을 인식하지 못했습니다. / 
즉, 인간의 판별 기준과 유사해졌습니다.

### 8.1.2 정확도를 높이려면

그렇다면 여기서 더 정확도를 높이고자 한다면 / 어떻게 해야 할까요? / 

MNIST 데이터에 대한 모델은 참으로 많습니다. / 
앞서 7장에서 구현한 / 모델 또한 정확도가 상당히 높았습니다.

이들을 살펴보면 / 대체로 은닉층의 수는 그리 많지 않습니다. / 그러나 앙상블 학습, 학습률 감소, / 데이터 확장 등의 다양한 기법을 이용하고 있습니다.

여기서 데이터 확장이란, 입력 이미지를 회전시키거나 / 이동시켜 데이터의 전체적인 형태에 대해 더 주목하게 하는 것입니다. / 특히 데이터가 적을 때 다양한 학습 데이터를 확보하게 하므로 / 효과적입니다.

나아가 조금 더 형태에 주목하고자 한다면, / 전체적인 밝기를 바꾼다든지 / 확대나 축소를 한다든지, 외형을 일부 조정한다든지 / 단순히 조금 다른 형태의 데이터를 이용하는 거지만, / 앞서 말한 것처럼 데이터를 확보하는데 있어 좋은 방법입니다.

다시 말해, 정확도를 높이고자 / 층을 깊게 쌓는 것 대신 다른 방법으로 / 정확도를 개선하고 있습니다.

### 8.1.3 깊게 하는 이유

그렇다면 신경망을 깊게 쌓는 이유는 뭘까요?

MNIST 말고 다른 대규모 이미지 인식 대회를 보면, / 층의 깊이에 비례해 정확도가 좋아집니다. / 
이번 장 처음에 실시했던 / MNIST 데이터에 대한 신경망을 다시 보면 알 수 있습니다.

먼저 깊지 않은 신경망의 7x7 연산에 대해 살펴보면, / 필터의 크기만큼 input 데이터의 영역의 영향을 받습니다. / 이는 3x3 filter와의 연산 2번으로 생각해볼 수 있습니다.

3x3 filter로 연산하는 걸 보면, / 처음에는 데이터를 중심으로 3x3 영역의 영향을 받고, / 그 다음 층에선 입력층을 기준으로 할 때 / 5x5의 영향을 받고, / 그 다음에선 7x7의 영향을 받는다는 것을 알 수 있습니다. / 이렇게 층이 깊어질수록 더 포괄적인, / 전체적인 이미지의 형태를 생각할 수 있게 되며, 

나아가 원래 7x7를 보려면 49개의 weight이 필요했지만, / 3x3x3이므로 27밖에 사용하지 않아 / 학습할 매개변수가 줄면서 더 빠르게 학습할 수 있다는 이점이 있습니다.

결과적으로 신경망을 깊게 하는 것은 / 적은 매개변수를 이용해 / 더 빠르고 좀 더 정확한 연산을 하기 위함이었습니다.

(네,) 이상으로 8 다시 1 절의 발표를 마칩니다.

---

# 전체적으로 보자면, 노이즈에 덜 민감하기 위해 많은 양의 데이터와 학습이 필요했음.
# 그러나 딥러닝은 사물이 가진 패턴, 작은 단위의 형태를 분석하고 조합하므로써 빠르게 학습할 수 있고, 노이즈와 같은 요인들을 배제하여 더 나은 결과를 내는 거지.
# 다시 말해 복잡한 문제를 몇 가지의 단순 문제의 조합으로 이해하는 거지.
# 나아가 CNN에서 초기 계층이 다양한 형태의 선을 인식하는 거라면, 그 다음 층에선 확인된 선을 조합해서 도형을 인식할 수 있으니까.
